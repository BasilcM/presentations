{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Introduction\n",
    "\n",
    "In this notebook, we will explore how to use AIF360 and AIX360 tool in creating an application for mortage approval\n",
    "\n",
    "Sections:\n",
    "    \n",
    "    1. File Read\n",
    "    2. Bias check and mitigation\n",
    "    3. Evaluate similar applicant\n",
    "    4. Contrastive explanations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "# \n",
    "from sklearn import preprocessing\n",
    "from keras.models import Sequential, Model, load_model, model_from_json\n",
    "from keras.layers import Dense\n",
    "from IPython.display import Markdown, display\n",
    "from sklearn.model_selection import train_test_split\n",
    "import tensorflow as tf\n",
    "#\n",
    "import aif360\n",
    "import aix360\n",
    "from aif360.metrics import BinaryLabelDatasetMetric\n",
    "from aif360.datasets import BinaryLabelDataset\n",
    "from aif360.algorithms.preprocessing.reweighing import Reweighing\n",
    "from aix360.algorithms.protodash import ProtodashExplainer\n",
    "from aix360.algorithms.contrastive import CEMExplainer, KerasClassifier\n",
    "#\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# File Read"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = pd.read_csv('preprocessed_data.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# AIF360"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Sex\n",
    "# 0 - Male \n",
    "# 1 - Female \n",
    "# Action Taken\n",
    "# 0 - Application denied \n",
    "# 1 - Purchased Loan "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "privileged_groups = [{'applicant_sex': 0}]\n",
    "unprivileged_groups = [{'applicant_sex': 1}]\n",
    "favorable_label = 1 \n",
    "unfavorable_label = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "BM_dataset = BinaryLabelDataset(favorable_label=1,\n",
    "                                unfavorable_label=0,\n",
    "                                df=data,\n",
    "                                label_names=['action_taken'],\n",
    "                                protected_attribute_names=['applicant_sex'],\n",
    "                                unprivileged_protected_attributes=[np.array([1])],\n",
    "                                privileged_protected_attributes=[np.array([0])])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "display(Markdown(\"#### Training Data Details\"))\n",
    "print(\"shape of the training dataset\", BM_dataset.features.shape)\n",
    "print(\"Training data favorable label\", BM_dataset.favorable_label)\n",
    "print(\"Training data unfavorable label\", BM_dataset.unfavorable_label)\n",
    "print(\"Training data protected attribute\", BM_dataset.protected_attribute_names)\n",
    "print(\"Training data privileged protected attribute (1:Male and 0:Female)\", \n",
    "      BM_dataset.privileged_protected_attributes)\n",
    "print(\"Training data unprivileged protected attribute (1:Male and 0:Female)\",\n",
    "      BM_dataset.unprivileged_protected_attributes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "metric_orig_train = BinaryLabelDatasetMetric(BM_dataset, \n",
    "                                             unprivileged_groups=unprivileged_groups,\n",
    "                                             privileged_groups=privileged_groups)\n",
    "print(\"Difference in mean outcomes between unprivileged and privileged groups = %f\" % \n",
    "      metric_orig_train.mean_difference())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "RW = Reweighing(unprivileged_groups=unprivileged_groups,\n",
    "               privileged_groups=privileged_groups)\n",
    "RW.fit(BM_dataset)\n",
    "train_tf_dataset = RW.transform(BM_dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "metric_orig_train_updated = BinaryLabelDatasetMetric(train_tf_dataset, \n",
    "                                             unprivileged_groups=unprivileged_groups,\n",
    "                                             privileged_groups=privileged_groups)\n",
    "\n",
    "print(\"Difference in mean outcomes between unprivileged and privileged groups = %f\"\n",
    "      % metric_orig_train_updated.mean_difference())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# AIX360"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dfTrain, dfTest, yTrain, yTest = train_test_split(train_tf_dataset.features, train_tf_dataset.labels, random_state=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dfTrain.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dfTest.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Simple neural network for the classification task\n",
    "def nn_small():\n",
    "    model = Sequential()\n",
    "    model.add(Dense(500, input_dim=14, activation='tanh'))\n",
    "    model.add(Dense(500, activation='relu'))\n",
    "    model.add(Dense(2))    \n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# loss function\n",
    "def fn(correct, predicted):\n",
    "    return tf.nn.softmax_cross_entropy_with_logits(labels=correct, logits=predicted)\n",
    "\n",
    "nn = nn_small()\n",
    "nn.compile(loss=fn, optimizer='adam', metrics=['accuracy'])\n",
    "nn.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# To train uncomment this cell\n",
    "\n",
    "# nn.fit(dfTrain, yTrain, batch_size=32, epochs=10, verbose=1, shuffle=False)\n",
    "\n",
    "# nn.save_weights(\"trained_model.h5\")     "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Load trained model weights \n",
    "nn.load_weights(\"trained_model.h5\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Display similar applicant user profiles and the extent to which they are similar to the chosen applicant as indicated by the last row in the table below labelled as \"Weight\"."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Explainer used:\n",
    "\n",
    "ProtodashExplainer provides exemplar-based explanations for summarizing datasets as well as explaining predictions made by an AI model. It employs a fast gradient based algorithm to find prototypes along with their (non-negative) importance weights. The algorithm minimizes the maximum mean discrepancy metric and has constant factor approximation guarantees for this weakly submodular function.\n",
    "\n",
    "\n",
    "More Details: \n",
    "https://aix360.readthedocs.io/en/latest/die.html#protodash-explainer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "p_train = nn.predict_classes(dfTrain) # Use trained neural network to predict train points\n",
    "p_train = p_train.reshape((p_train.shape[0],1))\n",
    "\n",
    "z_train = np.hstack((dfTrain, p_train)) # Store (normalized) instances that were predicted as Accepted\n",
    "z_train_good = z_train[z_train[:,-1]==1, :]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "idx = 86\n",
    "\n",
    "class_names = ['Denied', 'Accepted']\n",
    "\n",
    "X = dfTest[idx].reshape((1,) + dfTest[idx].shape)\n",
    "\n",
    "print(\"Chosen Sample:\", idx)\n",
    "print(\"Prediction made by the model:\", class_names[np.argmax(nn.predict_proba(X))])\n",
    "print(\"Prediction probabilities:\", nn.predict_proba(X))\n",
    "print(\"\")\n",
    "\n",
    "# attach the prediction made by the model to X\n",
    "X = np.hstack((X, nn.predict_classes(X).reshape((1,1))))\n",
    "\n",
    "Xun = dfTest[idx].reshape((1,) + dfTest[idx].shape) \n",
    "dfx = pd.DataFrame.from_records(Xun.astype('double')) # Create dataframe with original feature values\n",
    "dfx[14] = class_names[int(X[0, -1])]\n",
    "dfx.columns = data.columns\n",
    "dfx.transpose()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "explainer = ProtodashExplainer()\n",
    "(W, S, setValues) = explainer.explain(X, z_train_good, m=2) # Return weights W, Prototypes S and objective function values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dfs = pd.DataFrame.from_records(z_train_good[S, 0:-1].astype('double'))\n",
    "RP=[]\n",
    "for i in range(S.shape[0]):\n",
    "    RP.append(class_names[int(z_train_good[S[i], -1])]) # Append class names\n",
    "dfs[14] = RP\n",
    "dfs.columns = data.columns  \n",
    "dfs[\"Weight\"] = np.around(W, 2)/np.sum(np.around(W, 2)) # Calculate normalized importance weights\n",
    "dfs.transpose()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Contrastive explanations\n",
    "\n",
    "Example: when people ask for an explanation of an event -- the fact --- they (sometimes implicitly) are asking for an explanation relative to some contrast case; that is, \"Why P rather than Q?\"."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We now demonstrate how to compute contrastive explanations using AIX360 and how such explanations can help applicant understand the decisions made by AI models that approve or reject their applications.\n",
    "\n",
    "In this context, contrastive explanations provide information to applicants about what minimal changes to their profile would have changed the decision of the AI model from reject to accept or vice-versa (pertinent negatives).\n",
    "\n",
    "Terms:\n",
    "\n",
    "##### 1. Pertinent Negatives (PN) \n",
    "    \n",
    "   PNs identify a minimal set of features which if altered would change the classification of the original input. \n",
    "   \n",
    "   \n",
    "##### 2. Pertinent Positives (PP) : \n",
    "\n",
    "  PPs on the other hand identify a minimal set of features and their values that are sufficient to yield the original   input's classification. \n",
    "\n",
    "  For example, for an applicant whose HELOC application was approved, the explanation may say that even if the         number of satisfactory trades was reduced to a lower number, the loan would have still gotten through."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Let's explore PP"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Some interesting user samples to try: 8 9 11\n",
    "idx = 10\n",
    "\n",
    "X = dfTest[idx].reshape((1,) + dfTest[idx].shape)\n",
    "print(\"Computing PP for Sample:\", idx)\n",
    "print(\"Prediction made by the model:\", class_names[np.argmax(nn.predict_proba(X))])\n",
    "print(\"Prediction probabilities:\", nn.predict_proba(X))\n",
    "print(\"\")\n",
    "\n",
    "\n",
    "mymodel = KerasClassifier(nn)\n",
    "explainer = CEMExplainer(mymodel)\n",
    "\n",
    "arg_mode = 'PP' # Find pertinent positives\n",
    "arg_max_iter = 1000 # Maximum number of iterations to search for the optimal PN for given parameter settings\n",
    "arg_init_const = 10.0 # Initial coefficient value for main loss term that encourages class change\n",
    "arg_b = 9 # No. of updates to the coefficient of the main loss term\n",
    "arg_kappa = 0.1 # Minimum confidence gap between the PNs (changed) class probability and original class' probability\n",
    "arg_beta = 1e-1 # Controls sparsity of the solution (L1 loss)\n",
    "arg_gamma = 100 # Controls how much to adhere to a (optionally trained) auto-encoder\n",
    "my_AE_model = None # Pointer to an auto-encoder\n",
    "\n",
    "(adv_pp, delta_pp, info_pp) = explainer.explain_instance(X, arg_mode, my_AE_model, arg_kappa, arg_b,\n",
    "                                                         arg_max_iter, arg_init_const, arg_beta, arg_gamma)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Xpp = delta_pp\n",
    "classes = [ class_names[np.argmax(nn.predict_proba(X))], class_names[np.argmax(nn.predict_proba(Xpp))]]\n",
    "\n",
    "print(\"PP for Sample:\", idx)\n",
    "print(\"Prediction(Xpp) :\", class_names[np.argmax(nn.predict_proba(Xpp))])\n",
    "print(\"Prediction probabilities for Xpp:\", nn.predict_proba(Xpp))\n",
    "print(\"\")\n",
    "\n",
    "\n",
    "Xpp_re = X - adv_pp\n",
    "Xpp_re = np.around(Xpp_re.astype(np.double), 2)\n",
    "Xpp_re[Xpp_re < 1e-4] = 0\n",
    "\n",
    "X2 = np.vstack((X, Xpp_re))\n",
    "\n",
    "dfpp = pd.DataFrame.from_records(X2.astype('double')) # Showcase a dataframe for the original point and PP\n",
    "dfpp[23] = classes\n",
    "dfpp.columns = data.columns\n",
    "dfpp.rename(index={0:'X',1:'X_PP'}, inplace=True)\n",
    "dfppt = dfpp.transpose()\n",
    "\n",
    "def highlight_ce(s, col, ncols):\n",
    "    if (type(s[col]) != str):\n",
    "        if (s[col] > 0):\n",
    "            return(['background-color: yellow']*ncols)    \n",
    "    return(['background-color: white']*ncols)\n",
    "\n",
    "dfppt.style.apply(highlight_ce, col='X_PP', ncols=2, axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.rcdefaults()\n",
    "fi = abs(Xpp_re.astype('double'))/np.std(dfTrain.astype('double'), axis=0) # Compute PP feature importance\n",
    "    \n",
    "objects = data.columns[-2::-1]\n",
    "y_pos = np.arange(len(objects)) # Get input feature names\n",
    "performance = fi[0, -1::-1]\n",
    "\n",
    "plt.barh(y_pos, performance, align='center', alpha=0.5) # Bar chart\n",
    "plt.yticks(y_pos, objects) # Plot feature names on y-axis\n",
    "plt.xlabel('weight') #x-label\n",
    "plt.title('PP (feature importance)') # Figure heading\n",
    "\n",
    "plt.show()    # Display the feature importance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
